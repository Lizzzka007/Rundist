\chapter{Используемые математические объекты}

\subsection*{Робастный метод наименьших квадратов.}

Рассмотрим пример. Для оценки p неизвестных параметров $\theta_1,\; \dots ,\theta_p$ используется n наблюдений $y_1,\; \dots,y_n$, причем они связаны между собой следующим неравенством $\mathbf{y}=X\mathbf{\theta}+\mathbf{u}$, где элементы матрицы X суть известные коэффициенты, а $\mathbf{u}$ - вектор независимых случайных величин, имеющих(приблизительно) одинаковые функции распределения.

Тогда решение сводится к решению следующей задачи: 
$$\begin{gathered}
	|\mathbf{y}-X\mathbf{\theta}|^2 \rightarrow \min
\end{gathered}$$

Если матрица X - матрица полного ранга p, то $\hat \theta={(X^{T}X)}^{-1}X^T\mathbf{y}$, а оценки $\hat y_i$ будут высиляться по следующей формуле:
$$\begin{gathered}
	\hat{\mathbf{y}} = H\mathbf{y},
\end{gathered}$$

\noindentгде $H=X{(X^{T}X)}^{-1}X^T$.

\begin{definition}
	H - матрица подгонки.
\end{definition}

Таким образом, получили значения $\hat y_i$ и остатки $r_i=y_i-\hat y_i$.

Пусть $s_i$ - некоторая оценка стандартной ошибки наблюдений $r_i$ю

Метрически винзоризуем наблюдения $y_i$, заменяя их псевдонаблюдениями ${y_i}^{\ast}$:


${y_i}^{\ast}=
\left{
y_i\,,   \;   \;\; |r_i| \le cs_i \\
\hat y_i - cs_i\,, \;\; r_i<-cs_i \\
\hat y_i + cs_i\,, \;\; r_i>cs_i
\right.$
